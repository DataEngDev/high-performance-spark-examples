language: scala
sudo: false
cache:
  directories:
    - $HOME/.ivy2
    - $HOME/spark
    - $HOME/.cache/pip
    - $HOME/.sbt/launchers
    - $HOME/perl5
scala:
   - 2.11.6
jdk:
  - oraclejdk8
addons:
  apt:
    sources:
      - ubuntu-toolchain-r-test
    packages:
      - gfortran
      - gcc
      - binutils
      - python-pip
      - python-pandas
      - python-numpy
      - gfortran
      - cmake
      - perl
      - cpanminus
r_packages:
  - Imap
before_install:
  - pip install --user codecov unittest2 nose pep8 pylint --download-cache $HOME/.pip-cache
  - cpanm --force --local-lib $HOME/perl5  --quite --notest Pithub || cat ~/.cpanm/build.log
  - cd ./src/main/perl; cpanm --local-lib $HOME/perl5 --force --quiet --installdeps --notest .; cd ../../../
  - PATH="$HOME/perl5/bin${PATH:+:${PATH}}"; export PATH;
  - PERL5LIB=":$HOME/perl5/lib/perl5${PERL5LIB:+:${PERL5LIB}}"; export PERL5LIB;
  - PERL_LOCAL_LIB_ROOT="$HOME/perl5${PERL_LOCAL_LIB_ROOT:+:${PERL_LOCAL_LIB_ROOT}}"; export PERL_LOCAL_LIB_ROOT;
  - PERL_MB_OPT="--install_base \"$HOME/perl5\""; export PERL_MB_OPT;
  - PERL_MM_OPT="INSTALL_BASE=$HOME/perl5"; export PERL_MM_OPT;
script:
  - "export SPARK_CONF_DIR=./log4j/"
  - sbt clean coverage compile test
  - "[ -f spark] || mkdir spark && cd spark && wget http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz && cd .."
  - "tar -xf ./spark/spark-1.6.1-bin-hadoop2.6.tgz"
  - "export SPARK_HOME=`pwd`/spark-1.6.1-bin-hadoop2.6"
  - "export PYTHONPATH=$SPARK_HOME/python:`ls -1 $SPARK_HOME/python/lib/py4j-*-src.zip`:$PYTHONPATH"
  - "nosetests --with-doctest --doctest-options=+ELLIPSIS --logging-level=INFO --detailed-errors --verbosity=2 --with-coverage --cover-html-dir=./htmlcov"
  - $SPARK_HOME/spark-submit ./src/main/r/wc.R $SPARK_HOME/README.md
  - $SPARK_HOME/spark-submit ./src/main/r/dapply.R
after_success:
# For now no coverage report
  - codecov